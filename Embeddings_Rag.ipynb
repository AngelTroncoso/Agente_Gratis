{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelTroncoso/Agentes_Gratis/blob/main/Embeddings_Rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embeddings"
      ],
      "metadata": {
        "id": "3dOK3e_ilMRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Qué son los Embeddings?\n",
        "\n",
        "Imagina que tienes un diccionario. Cada palabra tiene su propia definición, pero también podríamos representar cada palabra como un punto en un espacio multidimensional. Palabras con significados similares estarían más cerca entre sí en este espacio, mientras que palabras con significados diferentes estarían más lejos.\n",
        "\n",
        "Los embeddings son precisamente esas representaciones vectoriales de palabras, frases o incluso documentos completos. Cada dimensión del vector captura una característica semántica o sintáctica del texto. Por ejemplo, en un espacio de embeddings, las palabras \"perro\" y \"gato\" estarían cercanas porque son ambos animales, mientras que \"perro\" y \"computadora\" estarían más lejos.\n",
        "\n",
        "¿Por qué son importantes los Embeddings en LangChain?\n",
        "\n",
        "Búsqueda Semántica: Al convertir el texto en vectores, podemos realizar búsquedas basadas en la semántica en lugar de solo en coincidencias exactas de palabras. Esto permite encontrar documentos relevantes incluso si no contienen las mismas palabras clave.\n",
        "Recomendaciones: Los embeddings se utilizan para recomendar artículos, productos o contenido similar a lo que el usuario ya ha visto.\n",
        "Clasificación: Ayudan a clasificar textos en categorías diferentes, como sentimientos (positivo, negativo, neutro) o temas (deportes, política, etc.).\n",
        "Generación de Texto: Los embeddings son fundamentales para modelos de lenguaje que generan texto, ya que permiten al modelo comprender el contexto y generar texto coherente.\n",
        "¿Cómo funcionan los Embeddings en LangChain?\n",
        "\n",
        "Conversión a Vectores: LangChain utiliza modelos preentrenados para convertir texto en vectores numéricos.\n",
        "Cálculo de Similitud: Se calcula la similitud entre los vectores, generalmente utilizando la distancia coseno.\n",
        "Tareas Basadas en Similitud: Una vez que tenemos los vectores, podemos realizar diversas tareas como búsqueda semántica, clustering, y más.\n",
        "Ejemplo Práctico:\n",
        "\n",
        "Imagina que tienes una base de datos de artículos sobre ciencia. Puedes convertir cada artículo en un vector. Luego, si un usuario busca información sobre \"inteligencia artificial\", puedes encontrar los artículos más similares buscando los vectores más cercanos al vector de la frase \"inteligencia artificial\".\n",
        "\n",
        "En resumen, los embeddings son una herramienta poderosa que permite a las máquinas comprender el significado del lenguaje y realizar tareas complejas como la búsqueda semántica, la clasificación y la generación de texto. LangChain facilita la integración de estos modelos en tus aplicaciones, permitiéndote aprovechar al máximo las capacidades de los modelos de lenguaje de gran tamaño."
      ],
      "metadata": {
        "id": "UzHuBV5mk_3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG permite a los modelos acceder a información externa actualizada, superando las limitaciones de los datos de entrenamiento estáticos.\n",
        "Mejora la precisión de las respuestas al basarlas en fuentes de información confiables y actuales.  \n",
        "Los LLMs pueden generar información incorrecta o \"alucinar\". RAG mitiga este problema al anclar las respuestas en datos recuperados.\n",
        "Permite adaptar las respuestas a contextos específicos o bases de conocimiento personalizadas.  \n",
        "Es crucial para aplicaciones en dominios especializados como medicina, leyes o finanzas.  \n",
        "Evita la necesidad de reentrenar constantemente modelos masivos para incorporar nueva información.  \n",
        "Permite usar modelos más pequeños y eficientes al complementarlos con conocimiento externo.  \n",
        "Potencia tareas como respuesta a preguntas, resumen de documentos y análisis de datos.  \n",
        "Especialmente útil en chatbots, asistentes virtuales y sistemas de soporte al cliente.  \n",
        "Ayuda al modelo a entender mejor el contexto de las consultas al proporcionar información relevante.  "
      ],
      "metadata": {
        "id": "rXvDndF0lEvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "FT1FtZR-KNwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwWuWmjAKGbi"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "sqd-HYFnKP4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        'Hola!',\n",
        "        'Holas, cómo estás?',\n",
        "        'Cual es tu nombre?',\n",
        "        'Me llamo Daniel',\n",
        "        'Hola Daniel'\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "F8K54LvPKRWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "Mh8JzcwtKTBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings[0])"
      ],
      "metadata": {
        "id": "s9i4ghJHKUlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query = embeddings_model.embed_query('Cual es el nombre mencionado en la conversación?')"
      ],
      "metadata": {
        "id": "dkDnO90DKWMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query"
      ],
      "metadata": {
        "id": "IpuGLkDzKXoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thmYPxzyKYxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen  \n",
        "Cuando trabajamos con herramientas como OpenAI o Gemini, accedemos a vastos modelos preentrenados con información de Internet. Sin embargo, esta información puede no estar actualizada o ser insuficiente para datos internos o específicos de una empresa, como información confidencial almacenada en sistemas ERP o CRM. Para solventar estas limitaciones, los embeddings juegan un rol crucial al permitir integrar y controlar información personalizada, ya sea actual o interna, proporcionando una representación semántica para una búsqueda eficiente y precisa.  \n",
        "\n",
        "¿Qué son los Embeddings?  \n",
        "Un embedding es una representación numérica o vectorial de un fragmento de texto. Esta representación captura el significado semántico del texto, lo que permite que textos similares tengan vectores cercanos en un espacio vectorial. Gracias a esta propiedad, los embeddings hacen posible:  \n",
        "\n",
        "Búsqueda semántica: Encontrar textos que sean similares en significado, no solo en palabras clave.  \n",
        "Operaciones matemáticas: Facilitar la búsqueda de datos relevantes mediante la comparación de vectores, haciendo posible encontrar el contenido que mejor se ajusta a la consulta del usuario.  \n",
        "Embeddings en LangChain  \n",
        "En LangChain, la clase Embeddings proporciona una interfaz estándar para interactuar con diversos modelos de embeddings, incluyendo proveedores populares como OpenAI y HuggingFace. Con esta herramienta, puedes transformar textos en vectores y, a partir de allí, realizar búsquedas semánticas o almacenar información en un Vector Store.  \n",
        "\n",
        "Flujo de Información en un Sistema con Embeddings\n",
        "El flujo típico de información cuando utilizamos embeddings en LangChain es el siguiente:  \n",
        "\n",
        "Consulta del usuario: El usuario plantea una pregunta o realiza una búsqueda.\n",
        "Búsqueda en la base vectorial: El sistema busca en una base vectorial previamente cargada, que contiene información personalizada (como documentos internos, bases de datos o información actualizada).  \n",
        "Aplicación del prompt: Un prompt guía al modelo de lenguaje (como GPT) en cómo devolver la respuesta.  \n",
        "Generación de la respuesta: El modelo, como OpenAI o HuggingFace, genera una respuesta basada en la información recuperada.  \n",
        "Entrega de la respuesta: El usuario recibe una respuesta personalizada, precisa y contextualizada.  \n",
        "¿Por qué utilizar Embeddings?  \n",
        "El uso de embeddings es crucial cuando necesitamos personalizar la información que un modelo de lenguaje maneja. Los embeddings permiten convertir un documento o fragmento de texto en una representación numérica, facilitando la recuperación de información específica y relevante. Esto es útil cuando trabajamos con datos que no están en los modelos preentrenados, como:\n",
        "\n",
        "Información confidencial de la empresa (datos internos).  \n",
        "Información actualizada o específica que debe consultarse en tiempo real.\n",
        "Datos almacenados en sistemas de gestión empresarial como ERP o CRM.\n",
        "Ejemplos de Aplicación\n",
        "Cargar un documento y convertirlo a vectores: Imagina que tienes una serie de documentos internos de tu empresa (por ejemplo, PDF, CSV, HTML) que quieres que el modelo de lenguaje entienda y utilice para responder preguntas. Primero, esos documentos deben ser vectorizados.  \n",
        "Realizar consultas semánticas: Una vez que tienes los textos convertidos a embeddings, puedes realizar consultas a esos vectores. Por ejemplo, si un usuario pregunta “¿Cuál es el nombre mencionado en la conversación?”, el sistema buscará en el espacio vectorial los textos que tengan una representación semánticamente cercana a la pregunta.\n",
        "Recuperación de Información y RAG\n",
        "Retrieved Augmented Generation (RAG) es un proceso en el que el modelo de lenguaje genera respuestas basadas en la información recuperada de una base de datos o almacén de vectores, en lugar de confiar únicamente en su conocimiento preentrenado. Este enfoque combina la recuperación de información relevante con la generación de respuestas contextualizadas y actualizadas.  \n",
        "\n",
        "Embeddings y Vector Stores  \n",
        "Una vez que tenemos nuestros textos convertidos en embeddings, estos se almacenan en un Vector Store. El Vector Store es un espacio de almacenamiento donde cada fragmento de texto se representa como un vector en un espacio multidimensional. Al realizar una consulta, el sistema busca los vectores más cercanos al vector de la consulta, proporcionando una respuesta basada en similitudes semánticas.  \n",
        "\n",
        "Beneficios de los Embeddings en el Contexto Empresarial\n",
        "Búsqueda semántica precisa: En lugar de depender de coincidencias exactas de palabras clave, los embeddings permiten realizar búsquedas basadas en el significado de las consultas, lo que es particularmente útil para encontrar información relevante en grandes volúmenes de datos.\n",
        "Personalización y control de la información: Los embeddings te permiten integrar información interna o específica en el flujo de trabajo, lo que ayuda a mejorar la precisión de las respuestas al usuario.\n",
        "Actualización de datos: Permiten trabajar con información nueva o actualizada, a diferencia de los modelos preentrenados, que pueden contener datos desactualizados.  \n",
        "Conclusión  \n",
        "El uso de embeddings en LangChain permite transformar textos en representaciones numéricas, facilitando la búsqueda y recuperación de información basada en similitud semántica. Esto es especialmente útil cuando trabajamos con datos internos, confidenciales o actualizados que no están presentes en los modelos preentrenados. Los embeddings, junto con herramientas como Vector Stores y procesos de Retrieved Augmented Generation, ofrecen una manera poderosa de personalizar la interacción con los usuarios y mejorar la precisión y relevancia de las respuestas."
      ],
      "metadata": {
        "id": "1cliFcaDkwfE"
      }
    }
  ]
}