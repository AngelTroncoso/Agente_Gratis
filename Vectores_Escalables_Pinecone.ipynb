{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelTroncoso/Agentes_Gratis/blob/main/Vectores_Escalables_Pinecone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VectorStore: Pinecone en LangChain\n",
        "Pinecone es otro popular servicio de almacenamiento y búsqueda de vectores utilizado en LangChain. Al igual que Chroma, ofrece una forma eficiente de almacenar y recuperar vectores numéricos, pero con algunas características distintivas.    \n",
        "\n",
        "Ventajas de Pinecone  \n",
        "Escalabilidad: Pinecone está diseñado para manejar grandes volúmenes de datos y puede escalar automáticamente según las necesidades.\n",
        "Gestión de Metadatos: Permite almacenar y buscar metadatos junto con los vectores, lo que puede ser útil para filtrar y organizar los resultados.\n",
        "Integración con Cloud Providers: Pinecone se integra fácilmente con los principales proveedores de cloud como AWS, GCP y Azure.\n",
        "API Amigable: Ofrece una API sencilla y bien documentada para interactuar con el servicio.  \n",
        "Cómo usar Pinecone en LangChain\n",
        "Configurar una Cuenta: Crea una cuenta en Pinecone y obtén las credenciales necesarias.  \n",
        "Crear una Colección: Crea una colección en Pinecone para almacenar tus vectores.\n",
        "Crear un VectorStore: En LangChain, utiliza la clase Pinecone para crear un VectorStore.  \n",
        "Agregar Vectores: Agrega tus vectores a la colección de Pinecone.\n",
        "Realizar Búsquedas: Utiliza el método similarity_search para buscar vectores similares a una consulta dada.  \n",
        "Ejemplo:  \n",
        "\n",
        "Pythonfrom langchain.vectorstores import Pinecone from langchain.embeddings import OpenAIEmbeddings  \n",
        "\n",
        "# Crear embeddings embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Crear un VectorStore vectordb = Pinecone.from_documents(     documents, embeddings, index_name=\"my_index\" )\n",
        "\n",
        "# Realizar una búsqueda docs = vectordb.similarity_search(\"What is the meaning of life?\") Usa el código con precaución.\n",
        "\n",
        "Casos de Uso\n",
        "Pinecone puede ser utilizado para una amplia variedad de aplicaciones, incluyendo:\n",
        "\n",
        "Búsqueda Semántica: Encontrar documentos relevantes basados en su contenido semántico.\n",
        "Recomendaciones: Sugerir productos, películas o artículos similares a los que el usuario ha visto o comprado.\n",
        "Análisis de Sentimientos: Clasificar textos según su sentimiento (positivo, negativo, neutro).\n",
        "Chatbots: Proporcionar respuestas relevantes a las preguntas de los usuarios.\n",
        "Conclusión\n",
        "Tanto Chroma como Pinecone son excelentes opciones para crear VectorStores en LangChain. La elección entre ellos dependerá de tus necesidades específicas, como el tamaño de los datos, la escalabilidad requerida y las características adicionales que necesites."
      ],
      "metadata": {
        "id": "myVQNCdEmYut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "FT1FtZR-KNwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwWuWmjAKGbi"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "sqd-HYFnKP4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        'Hola!',\n",
        "        'Holas, cómo estás?',\n",
        "        'Cual es tu nombre?',\n",
        "        'Me llamo Daniel',\n",
        "        'Hola Daniel'\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "F8K54LvPKRWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "Mh8JzcwtKTBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings[0])"
      ],
      "metadata": {
        "id": "s9i4ghJHKUlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query = embeddings_model.embed_query('Cual es el nombre mencionado en la conversación?')"
      ],
      "metadata": {
        "id": "dkDnO90DKWMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query"
      ],
      "metadata": {
        "id": "IpuGLkDzKXoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thmYPxzyKYxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen\n",
        "En el desarrollo de aplicaciones de inteligencia artificial, especialmente aquellas que dependen de modelos de lenguaje natural y embeddings, el uso de una VectorStore es esencial para realizar búsquedas semánticas eficientes. En esta clase, hemos explorado Pinecone, una solución en la nube diseñada para gestionar bases vectoriales a gran escala. A continuación, reforzaremos algunos conceptos clave sobre VectorStores, embeddings, y cómo integrarlos con herramientas como Pinecone y LangChain.\n",
        "\n",
        "¿Qué es un VectorStore?\n",
        "Un VectorStore es una base de datos que almacena representaciones numéricas, o vectores, de fragmentos de texto. Estos vectores se generan a partir de embeddings, que son representaciones matemáticas del significado semántico del texto. Los VectorStores permiten realizar búsquedas por similitud, recuperando los vectores más cercanos en significado a una consulta dada.\n",
        "\n",
        "Elementos Clave de un VectorStore\n",
        "Embeddings: Son vectores que representan el significado de un texto. Se generan a partir de modelos de lenguaje como OpenAI o HuggingFace. Fragmentos de texto similares tendrán vectores cercanos en un espacio vectorial.\n",
        "Almacenamiento: Los vectores generados se almacenan en una base vectorial junto con su metadata (información adicional como el origen del texto o categoría). Este almacenamiento es clave para realizar búsquedas eficaces basadas en similitud semántica.\n",
        "Consulta y Búsqueda por Similitud: Cuando realizamos una búsqueda, transformamos el texto de la consulta en un vector y lo comparamos con los vectores almacenados en el VectorStore. Los resultados más cercanos en el espacio vectorial son devueltos como respuestas.\n",
        "¿Qué es Pinecone?\n",
        "Pinecone es una solución gestionada en la nube que facilita el almacenamiento y la búsqueda de vectores a gran escala. Es ideal para aplicaciones que requieren rendimiento y escalabilidad en la gestión de bases vectoriales. Con Pinecone, los desarrolladores pueden centrarse en el desarrollo de aplicaciones mientras delegan la infraestructura de almacenamiento vectorial a un servicio especializado.\n",
        "\n",
        "Ventajas de Pinecone\n",
        "Escalabilidad: Gestiona eficientemente grandes volúmenes de datos.\n",
        "Fácil integración: Se integra con herramientas populares como LangChain, lo que permite una interacción fluida entre la generación de embeddings y su almacenamiento.\n",
        "Búsquedas eficientes: Permite búsquedas por similitud y otras métricas, optimizando la recuperación de información relevante.\n",
        "Creación de un VectorStore en Pinecone\n",
        "Al crear un index en Pinecone, es importante configurar ciertos parámetros:\n",
        "\n",
        "Dimensiones: Cada vector generado por un modelo de embeddings tiene un número de dimensiones (o componentes). Por ejemplo, un modelo de OpenAI puede generar vectores de 3072 dimensiones, lo que se debe especificar al configurar el index.\n",
        "Métricas de distancia: Pinecone permite definir la métrica con la que se calculará la similitud entre vectores, como la distancia euclidiana o la diferencia de cosenos.\n",
        "Proveedores Cloud: Puedes elegir el proveedor de nube (Azure, Google Cloud o AWS) y la región donde se almacenará el index, optimizando la latencia y el rendimiento de la búsqueda.\n",
        "Integración de Pinecone con LangChain\n",
        "LangChain facilita la integración con Pinecone para la gestión de embeddings y bases vectoriales. A través de la clase PineconeVectorStore, es posible crear, cargar y consultar vectores en Pinecone utilizando modelos de embeddings, como OpenAI.\n",
        "\n",
        "Flujo de Trabajo:\n",
        "Generación de Embeddings: Los fragmentos de texto se convierten en vectores utilizando un modelo de embeddings, como OpenAI Embeddings.\n",
        "Creación de la Base Vectorial: Se almacena la información vectorizada en Pinecone. Cada vector está asociado con su metadata, lo que permite realizar búsquedas avanzadas y filtradas.\n",
        "Consulta de la Base: Se puede hacer una búsqueda por similitud, transformando una nueva consulta en vector y buscando los documentos o fragmentos más cercanos en significado.\n",
        "Búsquedas en Pinecone\n",
        "Una de las funcionalidades clave en Pinecone es la búsqueda por similitud, que permite encontrar los documentos que están más relacionados semánticamente con una consulta. Sin embargo, no es la única técnica de búsqueda. Existen otros métodos que son útiles en diferentes contextos:\n",
        "\n",
        "Tipos de Búsqueda:\n",
        "Búsqueda por Similitud: Busca los vectores más cercanos en el espacio vectorial. Ideal para encontrar textos con significados similares.\n",
        "Búsqueda Filtrada: Filtra los resultados basados en metadata. Por ejemplo, puedes buscar solo entre los tweets almacenados o restringir la búsqueda a comentarios de una categoría específica.\n",
        "Búsqueda de Palabras Clave: Aunque Pinecone se enfoca en la búsqueda semántica, es posible combinar búsquedas tradicionales de palabras clave con búsquedas por embeddings para obtener resultados más precisos.\n",
        "Retos y Buenas Prácticas\n",
        "Control de Costos: Pinecone es una herramienta poderosa pero de paga, por lo que es esencial monitorear el uso de recursos y el número de documentos cargados para evitar costos excesivos.\n",
        "Optimización de Fragmentos: Al dividir documentos en fragmentos utilizando técnicas como Text Splitters, es importante optimizar el tamaño de los fragmentos y su superposición (overlap) para garantizar búsquedas eficientes sin pérdida de contexto."
      ],
      "metadata": {
        "id": "2qifb_F3mTOC"
      }
    }
  ]
}