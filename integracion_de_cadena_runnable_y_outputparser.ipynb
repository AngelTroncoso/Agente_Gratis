{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelTroncoso/Agentes_Gratis/blob/main/integracion_de_cadena_runnable_y_outputparser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integración de Cadenas en Proyectos : Runnable, OutputParser y Streaming"
      ],
      "metadata": {
        "id": "DpfLONDiFlEP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHQSnqcSFSIl"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "sequence = RunnableLambda(lambda x: x +1 ) | RunnableLambda(lambda x: x * 2 )"
      ],
      "metadata": {
        "id": "9GTWDdCUFmys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence.invoke(10)"
      ],
      "metadata": {
        "id": "y2lNOUG7FosD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = RunnableLambda(lambda x: x +1 ) | {\n",
        "    'index_1' : RunnableLambda(lambda x: x * 2 ),\n",
        "    'index_2' : RunnableLambda(lambda x: x * 5 )\n",
        "}"
      ],
      "metadata": {
        "id": "oAPgW9qMFqNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence.invoke(10)"
      ],
      "metadata": {
        "id": "y5di-HbtFrlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain_openai"
      ],
      "metadata": {
        "id": "IIOo61RLFs1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "aE7IqngwFunK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "pNyBuGHmFv2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "_QQYD-mIFxxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke_query = 'Tell me a joke'"
      ],
      "metadata": {
        "id": "WSOvHyTuFzPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = JsonOutputParser()"
      ],
      "metadata": {
        "id": "TVusM2bLF0m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = 'Answer the user query. \\n{format_instructions}\\n{query}',\n",
        "    input_variables = ['query'],\n",
        "    partial_variables = { 'format_instructions': parser.get_format_instructions()}\n",
        ")"
      ],
      "metadata": {
        "id": "pK7xjj2pF10n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | parser"
      ],
      "metadata": {
        "id": "z4SoFw61F3Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"query\": joke_query})"
      ],
      "metadata": {
        "id": "gTQnJjsUF49r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "for s in chain.stream({\"query\": joke_query}):\n",
        "  print(s)\n",
        "  time.sleep(0.3)"
      ],
      "metadata": {
        "id": "ujYj_071F6KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = []\n",
        "async for chunk in model.astream(joke_query):\n",
        "  chunks.append(chunk)\n",
        "  print(chunk.content, end='', flush=True)\n",
        "  time.sleep(0.3)"
      ],
      "metadata": {
        "id": "mR98aEOUF8jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen  \n",
        "Hemos explorado tres herramientas clave de LangChain que permiten construir flujos de trabajo más eficientes y escalables: Runnable, OutputParser, y Streaming. Estas herramientas ofrecen la capacidad de ejecutar tareas en secuencia o en paralelo, estructurar las respuestas en formatos como JSON, y ofrecer respuestas en tiempo real a los usuarios. Esto es esencial para desarrollar agentes inteligentes como chatbots avanzados, capaces de procesar grandes volúmenes de datos de manera rápida y organizada.\n",
        "\n",
        "1. Runnable: Ejecución en Secuencia o Paralelo\n",
        "Runnable es una herramienta que te permite encadenar y ejecutar tareas de forma secuencial o en paralelo. Esto resulta útil cuando tienes varias operaciones que deseas realizar de manera conjunta o cuando una secuencia de procesos depende de los resultados previos.\n",
        "\n",
        "Ejecución Secuencial: Permite que los procesos se ejecuten uno tras otro. Por ejemplo, el resultado de una operación matemática puede alimentar a la siguiente operación en la secuencia.\n",
        "Ejecución en Paralelo: En este modo, múltiples procesos pueden ejecutarse al mismo tiempo, lo que optimiza el tiempo de respuesta en tareas que no dependen entre sí.\n",
        "En el ejemplo de la clase, vimos cómo ejecutar una serie de funciones lambda donde el resultado de una función alimenta a la siguiente. Esta cadena de procesos es flexible y te permite manejar datos de manera eficiente.\n",
        "\n",
        "2. OutputParser: Convertir Salidas en Datos Estructurados\n",
        "OutputParser es una herramienta que convierte las respuestas generadas por el modelo en datos estructurados. Esto es particularmente útil cuando necesitas una salida en un formato específico, como JSON, en lugar de una simple cadena de texto.\n",
        "\n",
        "JSON OutputParser: Este parser convierte la respuesta del modelo en un formato JSON, lo que facilita su uso en aplicaciones que requieren respuestas estructuradas y organizadas.\n",
        "Aplicación en Chatbots: Al crear agentes que gestionen grandes cantidades de datos o necesiten respuestas estructuradas, como resúmenes, informes o análisis, OutputParser permite que las respuestas sean fácilmente procesables por otros sistemas.\n",
        "En el ejemplo de la clase, el JSON OutputParser se utilizó para devolver la respuesta del modelo en formato JSON, lo que permite que las respuestas sean más fáciles de interpretar y reutilizar.\n",
        "\n",
        "3. Streaming: Resultados en Tiempo Real\n",
        "Streaming permite que las respuestas generadas por el modelo se transmitan en tiempo real, lo que es útil en escenarios donde los usuarios no quieren esperar a que se genere una respuesta completa. Esto es similar a cómo funciona ChatGPT, donde ves la respuesta construirse a medida que se genera.\n",
        "\n",
        "Experiencia de Usuario Mejorada: En lugar de esperar a que el modelo genere la respuesta completa, el usuario ve cómo se desarrolla la respuesta en tiempo real, lo que ofrece una experiencia más interactiva y satisfactoria.\n",
        "Aplicaciones en Chatbots: Esta técnica es especialmente útil en chatbots donde la interacción fluida es clave, y los usuarios quieren ver respuestas rápidas y en tiempo real.\n",
        "En la clase, se utilizó Streaming para enviar la respuesta del modelo en fragmentos, lo que permite que el texto se genere de manera progresiva y el usuario vea cómo se construye la respuesta sin tener que esperar demasiado.\n",
        "\n",
        "4. Integración de Runnable, OutputParser y Streaming\n",
        "La combinación de estas herramientas permite construir agentes que no solo ejecuten tareas de manera eficiente, sino que también entreguen respuestas precisas y en tiempo real. Un agente que utiliza Runnable puede manejar múltiples tareas de manera simultánea o secuencial, y con OutputParser puedes asegurarte de que las respuestas del modelo tengan el formato adecuado. Streaming, por su parte, mejora la experiencia del usuario al ofrecer resultados inmediatos.\n",
        "\n",
        "Ejemplo de Aplicación: Un chatbot financiero podría utilizar Runnable para procesar consultas de varios usuarios en paralelo, aplicar un OutputParser para entregar informes financieros en formato JSON, y utilizar Streaming para mostrar actualizaciones en tiempo real sobre los precios de las acciones.\n",
        "5. Aplicaciones Prácticas\n",
        "Con estas herramientas, puedes crear aplicaciones como:\n",
        "\n",
        "Asistentes Virtuales: Que manejan múltiples consultas simultáneamente, proporcionando respuestas personalizadas y organizadas en tiempo real.\n",
        "Sistemas de Atención al Cliente: Que pueden estructurar respuestas y generar informes de manera instantánea, mejorando la eficiencia en la resolución de problemas.\n",
        "Plataformas Educativas: Que permiten a los estudiantes obtener respuestas en tiempo real mientras realizan tareas y reciben retroalimentación instantánea.\n",
        "6. Conclusión\n",
        "En esta clase, vimos cómo integrar Runnable, OutputParser, y Streaming para construir aplicaciones más potentes y eficientes utilizando LangChain. Estas herramientas ofrecen una forma flexible y escalable de gestionar tareas, estructurar respuestas y mejorar la experiencia del usuario con resultados en tiempo real.\n",
        "\n",
        "El reto que te dejo es que explores cómo combinar estas herramientas en un proyecto personal. Intenta construir una cadena que utilice Runnable para ejecutar múltiples tareas, estructure las salidas en formato JSON con OutputParser, y utilice Streaming para mostrar las respuestas en tiempo real a los usuarios. ¡Déjame saber en los comentarios cómo te fue!"
      ],
      "metadata": {
        "id": "bY_dIp9WXisg"
      }
    }
  ]
}