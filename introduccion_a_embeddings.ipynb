{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelTroncoso/Agentes_Gratis/blob/main/introduccion_a_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Qué es un VectorStore?\n",
        "En el contexto de LangChain, un VectorStore es una base de datos especializada diseñada para almacenar y recuperar vectores numéricos. Estos vectores, como ya hemos visto, son representaciones numéricas de textos, imágenes, o cualquier otro tipo de dato que pueda ser codificado en un espacio vectorial.\n",
        "\n",
        "Un VectorStore es crucial para:\n",
        "\n",
        "Búsqueda Semántica: Permite buscar información basada en el significado en lugar de coincidencia exacta de palabras clave.\n",
        "Recuperación de Información: Ayuda a encontrar documentos relevantes en grandes conjuntos de datos.\n",
        "Recomendaciones: Puede sugerir elementos similares a los que el usuario ha interactuado previamente.\n",
        "¿Por qué Chroma?\n",
        "Chroma es una de las opciones más populares para crear VectorStores en LangChain. Ofrece una serie de ventajas:\n",
        "\n",
        "Rendimiento: Está optimizado para realizar búsquedas de similitud en grandes conjuntos de vectores de manera eficiente.\n",
        "Flexibilidad: Permite almacenar diferentes tipos de datos y ofrece múltiples opciones de indexación.\n",
        "Integración con LangChain: Se integra de manera nativa con LangChain, facilitando su uso en proyectos de procesamiento de lenguaje natural.\n",
        "Escalabilidad: Puede manejar grandes volúmenes de datos y escalar fácilmente para adaptarse a las necesidades crecientes.\n",
        "¿Cómo funciona Chroma en LangChain?\n",
        "Creación del VectorStore: Se crea una instancia de Chroma y se configura con los parámetros necesarios.\n",
        "Adición de Vectores: Los vectores, junto con su metadata (por ejemplo, el texto original), se añaden al VectorStore.\n",
        "Búsqueda: Cuando se realiza una búsqueda, se convierte la consulta en un vector y se busca los vectores más cercanos en el VectorStore.\n",
        "Recuperación de Información: Se recuperan los documentos asociados a los vectores más cercanos, que son los más relevantes para la consulta.\n",
        "Ejemplo básico:\n",
        "\n",
        "Pythonfrom langchain.vectorstores import Chroma from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Crear embeddings embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Crear un VectorStore vectordb = Chroma.from_documents(     documents, embeddings, collection_name=\"my_collection\" )\n",
        "\n",
        "# Realizar una búsqueda docs = vectordb.similarity_search(\"What is the meaning of life?\") Usa el código con precaución.\n",
        "\n",
        "Casos de Uso\n",
        "Chatbots: Para encontrar respuestas relevantes a preguntas de los usuarios.\n",
        "Sistemas de Recomendación: Para sugerir productos, películas o noticias basadas en las preferencias del usuario.\n",
        "Búsqueda Semántica: Para buscar información en grandes bases de datos de texto.\n",
        "Resumen Automático: Para identificar las partes más relevantes de un documento.\n",
        "Conclusiones\n",
        "Chroma es una herramienta poderosa y flexible para crear VectorStores en LangChain. Al permitir la búsqueda semántica y la recuperación de información basada en similitud, abre un mundo de posibilidades para aplicaciones de procesamiento de lenguaje natural."
      ],
      "metadata": {
        "id": "HQFkVwlBjuZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "FT1FtZR-KNwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwWuWmjAKGbi"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "id": "sqd-HYFnKP4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings()\n",
        "embeddings = embeddings_model.embed_documents(\n",
        "    [\n",
        "        'Hola!',\n",
        "        'Holas, cómo estás?',\n",
        "        'Cual es tu nombre?',\n",
        "        'Me llamo Daniel',\n",
        "        'Hola Daniel'\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "F8K54LvPKRWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "Mh8JzcwtKTBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings[0])"
      ],
      "metadata": {
        "id": "s9i4ghJHKUlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query = embeddings_model.embed_query('Cual es el nombre mencionado en la conversación?')"
      ],
      "metadata": {
        "id": "dkDnO90DKWMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_query"
      ],
      "metadata": {
        "id": "IpuGLkDzKXoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "thmYPxzyKYxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen\n",
        "Un VectorStore es una estructura diseñada para almacenar y gestionar vectores de texto, también conocidos como embeddings. Los embeddings son representaciones numéricas de fragmentos de texto que capturan su significado semántico, permitiendo que los textos con significados similares tengan vectores cercanos entre sí en un espacio vectorial. Esta capacidad es esencial para realizar búsquedas semánticas basadas en similitud, lo que facilita la recuperación de información relevante.\n",
        "\n",
        "¿Cómo Funciona un VectorStore?\n",
        "Generación de Embeddings: Primero, el texto se convierte en un vector mediante un modelo de embeddings, como los que ofrece OpenAI o HuggingFace.\n",
        "Almacenamiento de Vectores: Estos vectores, junto con metadatos (como el origen del documento o la categoría), se almacenan en un VectorStore.\n",
        "Consulta y Búsqueda: Cuando se recibe una nueva consulta (query), esta también se transforma en un vector y se compara con los vectores ya almacenados para encontrar aquellos que sean más cercanos en significado.\n",
        "Chroma: Un VectorStore Open Source\n",
        "En esta clase exploramos Chroma, una tecnología open source que permite gestionar bases vectoriales. La integración entre LangChain y Chroma facilita la creación y manejo de estas bases. Con Chroma, puedes almacenar y buscar vectores creados a partir de texto, ya sea de archivos CSV, PDF, HTML o incluso entradas directas como comentarios en redes sociales.\n",
        "\n",
        "Flujo de Trabajo de un VectorStore con Embeddings\n",
        "Transformación del Texto: El texto se convierte en un vector utilizando un modelo de embeddings.\n",
        "Almacenamiento: Los vectores generados, junto con su metadata (como fuente, categoría o ID), se almacenan en una colección dentro de la base vectorial.\n",
        "Búsqueda por Similitud: Cuando realizamos una búsqueda, el texto de la consulta se convierte en un vector y se compara con los vectores almacenados en la base. Los resultados son los vectores más cercanos, según la métrica de similitud.\n",
        "Creación de la Base Vectorial en Chroma\n",
        "Instalación: La instalación de Chroma es simple, se realiza a través de pip install chromadb.\n",
        "Integración con LangChain: Chroma se integra con LangChain para permitir la gestión de los vectores. En LangChain, puedes utilizar modelos como OpenAI para generar embeddings y luego almacenar esos vectores en una base creada con Chroma.\n",
        "Documentos y Metadatos: Los documentos se convierten en embeddings y se almacenan con metadatos adicionales que ayudan a identificar y filtrar la información durante las búsquedas.\n",
        "Ejemplo Práctico\n",
        "Un caso de uso que exploramos es el almacenamiento de comentarios de redes sociales (por ejemplo, tweets) en una base vectorial. Cada comentario se convierte en un vector, y se le asigna metadata como la fuente (Twitter) y un ID único. Esto nos permite posteriormente realizar búsquedas entre estos comentarios, recuperando aquellos que son semánticamente similares a una nueva consulta.\n",
        "\n",
        "Por ejemplo, si tenemos una base vectorial con comentarios relacionados a LangChain, cuando un usuario ingresa una consulta sobre “Large Language Models”, la búsqueda devolverá los comentarios más cercanos en significado, mostrando aquellos que mencionan temas relacionados, como “LangChain” o “Language Models”.\n",
        "\n",
        "Búsqueda por Similitud en Chroma\n",
        "La búsqueda por similitud en un VectorStore de Chroma sigue estos pasos:\n",
        "\n",
        "Query: El usuario ingresa una consulta en formato de texto.\n",
        "Conversión a Vector: La consulta se transforma en un vector utilizando el mismo modelo de embeddings que se usó para los documentos.\n",
        "Búsqueda en la Base Vectorial: El sistema compara el vector de la consulta con los vectores almacenados y recupera los más cercanos en función de su similitud.\n",
        "Filtrado por Metadata: Puedes aplicar filtros para restringir la búsqueda a ciertos tipos de documentos o fuentes, como comentarios de Twitter o posts de Facebook.\n",
        "Beneficios de Utilizar Chroma con LangChain\n",
        "Open Source: Chroma es una solución de código abierto que permite gestionar bases vectoriales sin depender de servicios pagos.\n",
        "Versatilidad: Puedes almacenar y buscar vectores generados a partir de múltiples tipos de datos, incluyendo archivos CSV, PDFs, HTML, y más.\n",
        "Búsqueda Semántica: Al trabajar con embeddings, las búsquedas no se limitan a palabras clave exactas, sino que buscan similitudes en el significado, proporcionando resultados más relevantes.\n",
        "Facilidad de Integración: LangChain facilita la integración con Chroma, lo que simplifica el proceso de crear y gestionar bases vectoriales."
      ],
      "metadata": {
        "id": "fyfV-G2zjoDt"
      }
    }
  ]
}