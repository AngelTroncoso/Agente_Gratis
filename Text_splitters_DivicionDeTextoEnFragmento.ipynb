{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelTroncoso/Agentes_Gratis/blob/main/Text_splitters_DivicionDeTextoEnFragmento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Splitters en LangChain son herramientas esenciales para dividir grandes trozos de texto en fragmentos más pequeños y manejables. Esto es crucial para trabajar con modelos de lenguaje de gran tamaño (LLMs), ya que estos modelos suelen tener limitaciones en la cantidad de texto que pueden procesar a la vez.\n",
        "\n",
        "Tipos de Text Splitters en LangChain\n",
        "LangChain proporciona varios tipos de text splitters, cada uno con sus propias características y ventajas:\n",
        "\n",
        "CharacterTextSplitter: Divide el texto en fragmentos de un tamaño específico en caracteres.\n",
        "SentenceTextSplitter: Divide el texto en fragmentos basados en la detección de oraciones.\n",
        "RecursiveCharacterTextSplitter: Divide el texto recursivamente, primero en fragmentos más grandes y luego en fragmentos más pequeños si es necesario.\n",
        "TokenTextSplitter: Divide el texto en fragmentos basados en tokens, utilizando un tokenizador subyacente.\n",
        "RegexPatternTextSplitter: Divide el texto según patrones definidos por expresiones regulares.\n",
        "Ejemplo de Uso\n",
        "Pythonfrom langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text = \"This is a long text that needs to be split into smaller chunks.\"\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=50) chunks = text_splitter.split_text(text)\n",
        "\n",
        "print(chunks) Usa el código con precaución.\n",
        "\n",
        "Consideraciones Importantes\n",
        "Tamaño de los fragmentos: El tamaño óptimo de los fragmentos puede variar según el modelo de lenguaje y la tarea específica.\n",
        "Overlap: Puedes especificar un overlap entre los fragmentos para evitar perder contexto.\n",
        "Tokenización: Si estás utilizando un TokenTextSplitter, asegúrate de que el tokenizador utilizado sea compatible con el modelo de lenguaje.\n",
        "Personalización: Puedes personalizar los text splitters para satisfacer tus necesidades específicas, como utilizando expresiones regulares o funciones personalizadas.\n",
        "Casos de Uso\n",
        "Procesamiento de documentos largos: Dividir documentos largos en fragmentos más pequeños para procesarlos de manera eficiente.\n",
        "Preparación de datos para entrenamiento: Dividir grandes conjuntos de datos de texto en fragmentos para entrenar modelos de lenguaje.\n",
        "Interacción con modelos de lenguaje: Dividir las consultas del usuario en fragmentos más pequeños para evitar problemas de longitud.\n",
        "Conclusión\n",
        "Los text splitters son una herramienta fundamental en LangChain para trabajar con textos de diferentes tamaños y formatos. Al utilizarlos de manera efectiva, puedes mejorar la eficiencia y la precisión de tus aplicaciones basadas en modelos de lenguaje."
      ],
      "metadata": {
        "id": "aTv-S2CuixnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitters"
      ],
      "metadata": {
        "id": "B1QUU31WJlrV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2NQFqUyJYLc"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "H1wfALGpJn89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('example.txt') as f:\n",
        "  state_of_the_union = f.read()"
      ],
      "metadata": {
        "id": "xzzIICa1JpMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=10,\n",
        "    chunk_overlap=2,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "SS4wtw-PJqe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text_splitter.create_documents([state_of_the_union])"
      ],
      "metadata": {
        "id": "qMCQcYFjJr0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "wx-04LFQJtRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[0]"
      ],
      "metadata": {
        "id": "dHglcWr6Jun9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[1]"
      ],
      "metadata": {
        "id": "jon0FDDXJygl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumen\n",
        "Cuando trabajamos con textos largos en LangChain, como libros, artículos o transcripciones, nos enfrentamos a la limitación de los modelos de lenguaje respecto a la cantidad de información que pueden procesar en una sola vez. Aquí es donde los TextSplitters juegan un papel fundamental. Su propósito es dividir el texto en fragmentos manejables, asegurando que el contenido siga siendo coherente y utilizable para tareas posteriores, como la generación de respuestas, búsqueda de información o resúmenes.\n",
        "\n",
        "A continuación, veremos algunos conceptos clave que te ayudarán a entender el uso y la importancia de los TextSplitters.\n",
        "\n",
        "1. ¿Qué son los TextSplitters?\n",
        "Los TextSplitters son herramientas que dividen un texto largo en fragmentos más pequeños, también llamados chunks, para que los modelos de lenguaje puedan procesarlos sin exceder los límites de tokens. Además de respetar estos límites, un buen TextSplitter mantiene la coherencia semántica entre los fragmentos, asegurando que la información relevante no se pierda en la división.\n",
        "\n",
        "2. Características principales de los TextSplitters\n",
        "a) Límite de tokens\n",
        "Los modelos de lenguaje como GPT tienen un número máximo de tokens que pueden procesar en una sola entrada. Un token puede ser una palabra, parte de una palabra o incluso un símbolo. Los TextSplitters dividen el texto en fragmentos que se ajustan a este límite de tokens para que el modelo pueda manejar la entrada sin problemas.\n",
        "\n",
        "b) Contexto y superposición (Overlap)\n",
        "Cuando dividimos el texto, es importante que no se pierda el contexto entre fragmentos. Para mantener la continuidad, los TextSplitters permiten superponer (overlap) un número de caracteres o tokens entre los chunks, lo que asegura que la transición entre fragmentos mantenga la coherencia de la información.\n",
        "\n",
        "Ejemplo: Si un chunk finaliza en un punto importante, el siguiente chunk puede superponer parte del final del chunk anterior para asegurar que el contexto se mantenga.\n",
        "\n",
        "c) Semántica\n",
        "Dividir el texto no solo implica contar caracteres o tokens, sino asegurarse de que cada fragmento tenga un sentido semántico completo. Los TextSplitters pueden ajustarse para que cada chunk contenga frases o párrafos completos, asegurando que la información no quede incompleta o malinterpretada.\n",
        "\n",
        "3. Tipos de TextSplitters\n",
        "En LangChain, existen varias formas de dividir el texto, dependiendo de las necesidades del proyecto. Algunos de los tipos más comunes son:\n",
        "\n",
        "a) División por caracteres\n",
        "Este tipo de TextSplitter divide el texto simplemente contando un número fijo de caracteres. Aunque es útil para dividir textos largos rápidamente, puede no ser el mejor método si se necesita mantener la coherencia semántica, ya que puede cortar oraciones o párrafos a la mitad.\n",
        "\n",
        "b) División por tokens\n",
        "Este método es ideal para trabajar con modelos de lenguaje que tienen límites específicos de tokens, ya que ajusta los fragmentos según la cantidad de tokens, asegurando que no se exceda el límite del modelo. Es útil cuando se trabaja con grandes volúmenes de texto y se necesita dividir el contenido en partes pequeñas procesables.\n",
        "\n",
        "c) División por semántica o separadores personalizados\n",
        "A veces, es importante que los fragmentos mantengan una estructura lógica o semántica, como dividir un texto por párrafos, secciones o frases completas. Esto se puede lograr utilizando separadores personalizados o expresiones regulares que identifiquen patrones en el texto (por ejemplo, dividir por signos de puntuación o saltos de línea).\n",
        "\n",
        "4. Parámetros Clave al Usar TextSplitters\n",
        "a) Chunk size\n",
        "Define el número de caracteres o tokens en cada fragmento. Este parámetro es clave cuando trabajas con límites específicos, como los tokens máximos que un modelo puede procesar.\n",
        "\n",
        "b) Overlap\n",
        "Este parámetro define cuántos caracteres o tokens de un chunk se superponen con el chunk anterior o siguiente. Esto es útil para mantener el contexto y asegurar que la división no corte partes cruciales del texto.\n",
        "\n",
        "c) Length function\n",
        "Determina cómo se calcula el tamaño de cada chunk, ya sea por longitud de caracteres, tokens u otros criterios. Puedes ajustar esta función dependiendo de las necesidades de tu proyecto.\n",
        "\n",
        "d) Separadores personalizados\n",
        "Puedes utilizar expresiones regulares o definiciones específicas para asegurarte de que los fragmentos se dividan de manera lógica y semánticamente coherente, basándose en frases completas, signos de puntuación o saltos de párrafo.\n",
        "\n",
        "5. Aplicaciones de TextSplitters\n",
        "Los TextSplitters son útiles en varios escenarios, incluyendo:\n",
        "\n",
        "Análisis de documentos largos: Cuando se procesan libros, artículos de investigación o transcripciones extensas.\n",
        "Búsqueda semántica: Dividir grandes volúmenes de texto para realizar búsquedas más precisas y eficientes.\n",
        "Resúmenes: Extraer los puntos más importantes de cada chunk para generar resúmenes coherentes y completos.\n",
        "Procesamiento de lenguaje natural (NLP): Permiten ajustar los fragmentos para que el modelo de lenguaje pueda entender el contexto de manera más eficaz.\n",
        "Conclusión\n",
        "Los TextSplitters son herramientas indispensables para trabajar con documentos largos en LangChain, ya que permiten dividir el contenido en fragmentos más pequeños y manejables, mientras se respeta la coherencia semántica y los límites de tokens del modelo. Esto asegura que los modelos de lenguaje, como GPT, puedan procesar la información de manera efectiva sin perder el contexto."
      ],
      "metadata": {
        "id": "txd26Vk-ipL2"
      }
    }
  ]
}